# Eigen Knowledge Distillation Python Library
This repository is meant to to provide an end-to-end implementation of Knowledge Distillation (KD) techniques (offline, online, self) for model compression and optimization. The goal is to democratise ML model inference through distillation. 

## How Eigen Can Help You with Knowledge Distillation  

### 1ï¸âƒ£ Deploying AI on Mobile ğŸ“±  
**Have a new vision segmentation model but donâ€™t want it to drain memory or battery on mobile devices?**  
ğŸ’¡ Distill the model down to a smaller architecture using **Eigenâ€™s offline distillation**, keeping accuracy while reducing compute costs.  

### 2ï¸âƒ£ Making LLMs Cheaper & Faster ğŸ§ âš¡  
**Have a powerful LLM but itâ€™s too slow and expensive to deploy in production?**  
ğŸ’¡ Use **Eigenâ€™s online distillation** to train a smaller student LLM in real-time while retaining knowledge from the original model.  

### 3ï¸âƒ£ Optimizing Edge AI for IoT & Robotics ğŸ¤–ğŸŒ  
**Want to run an object detection model on an edge device but canâ€™t afford a massive YOLO or Faster R-CNN?**  
ğŸ’¡ Apply **feature-based distillation** with Eigen to compress the model while preserving detection accuracy.  

### 4ï¸âƒ£ Speeding Up Vision Transformers (ViTs) ğŸ–¼ï¸âš¡  
**Training a ViT but need efficient inference without losing too much performance?**  
ğŸ’¡ Use **self-distillation** to refine the modelâ€™s internal representations, reducing redundancy while improving feature extraction.  

### 5ï¸âƒ£ Accelerating Generative AI ğŸ¨ğŸ’¨  
**Want faster inference for a diffusion model or GAN without sacrificing image quality?**  
ğŸ’¡ Use **contrastive distillation** in Eigen to train a lightweight generative model that runs faster while keeping high visual fidelity.  

## Features 
- [ ] Offline Distillation Pipeline
  - [ ] Probability Output - Student learns the teacher's soft logits 
  - [ ] Intermediate Feature Maps - Student mimics the teachers's activations
  - [ ] Feature Map Relations - Student captures the relationships between teacher's feature maps
  - [ ] Attention Transfer
  - [ ] ViT to CNN Transfer 
  - [ ] Multi-Teacher Support
  - [ ] Customizable Loss Functions

## Installation
To install the library, you can clone this repository and install the dependencies using pip:
```bash
git clone https://github.com/0xd1rac/eigen-distill-lib.git
cd eigen-distill-lib
pip install -r requirements.txt
```

## Usage 
### 1. Offline Disillation: Soft Output Strategy 
```python

```
