# Eigen.AI Knowledge Distillation Python Library
This repository provides an end-to-end implementation of Knowledge Distillation (KD) techniques (offline, online, self) for model compression and optimization. Check out .md for current goals of this prject.

## Features 
- [ ] Offline Distillation Pipeline
  - [ ] Probability Output - Student learns the teacher's soft logits 
  - [ ] Intermediate Feature Maps - Student mimics the teachers's activations
  - [ ] Feature Map Relations - Student captures the relationships between teacher's feature maps
  - [ ] Attention Transfer
  - [ ] ViT to CNN Transfer 
  - [ ] Multi-Teacher Support
  - [ ] Customizable Loss Functions

## Installation
To install the library, you can clone this repository and install the dependencies using pip:
```bash
git clone https://github.com/0xd1rac/eigen-distill-lib.git
cd eigen-distill-lib
pip install -r requirements.txt
```

## Usage 
### 1. Offline Disillation: Soft Output Strategy 
```python

```
